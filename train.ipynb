{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab42bb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cc555",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## SPLITTING THE DATA\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "SOURCE_DIR = \"/Users/niakumar/Documents/Endometrial_US_DATA/TRAIN/Benign\"\n",
    "TARGET_DIR = \"/Users/niakumar/Documents/Ultrasound_Split\"\n",
    "CLASS_NAME = \"Benign\"\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# destination paths\n",
    "\n",
    "train_dir = Path(TARGET_DIR) / \"train\" / CLASS_NAME\n",
    "val_dir = Path(TARGET_DIR) / \"validate\" / CLASS_NAME\n",
    "train_dir.mkdir(parents =True, exist_ok=True)\n",
    "val_dir.mkdir(parents =True, exist_ok=True)\n",
    "\n",
    "# split files\n",
    "\n",
    "random.seed(SEED)\n",
    "all_files = [f for f in os.listdir(SOURCE_DIR) if f.lower().endswith(('.bmp', '.png', '.jpg', '.jpeg'))]\n",
    "random.shuffle(all_files)\n",
    "val_count = int(len(all_files) * VAL_SPLIT)\n",
    "val_files = set(all_files[:val_count])\n",
    "\n",
    "for fname in all_files:\n",
    "    src_path = os.path.join(SOURCE_DIR, fname)\n",
    "    dst_path = os.path.join(val_dir if fname in val_files else train_dir, fname)\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "\n",
    "print(f\"Total images: {len(all_files)}\")\n",
    "print(f\"Copied to:\")\n",
    "print(f\" ->Val: {len(val_files)} -> {val_dir}\")\n",
    "print(f\" ->Train: {len(all_files) - len(val_files)} -> {train_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b5be0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.ImageFolder(Path(TARGET_DIR) / \"train\", transform=transform)\n",
    "val_data = datasets.ImageFolder(Path(TARGET_DIR) / \"validate\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa11119",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "## ADAMW OPTIMIZER WITH COSINE ANNEALING SCHEDULER ##n\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.4, 0.6]).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeba334",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(30):  # or more\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df9e3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {correct / total:.2%}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'resnet_ultrasound.pt')\n",
    "print(\"âœ… Model saved as resnet_ultrasound.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2fb78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- UMAP Plotting Function ---\n",
    "def plot_umap(embeddings, labels, plot_3d=True, title=\"UMAP Projection\"):\n",
    "    reducer = umap.UMAP(n_components=3 if plot_3d else 2, random_state=42)\n",
    "    embedding_umap = reducer.fit_transform(embeddings)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    cmap_choice = 'tab10'  # Better for class contrast\n",
    "    if plot_3d:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(\n",
    "            embedding_umap[:, 0], embedding_umap[:, 1], embedding_umap[:, 2],\n",
    "            c=labels, cmap=cmap_choice, s=40, alpha=0.9\n",
    "        )\n",
    "    else:\n",
    "        scatter = plt.scatter(\n",
    "            embedding_umap[:, 0], embedding_umap[:, 1],\n",
    "            c=labels, cmap=cmap_choice, s=40, alpha=0.9\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "\n",
    "# --- Use the pretrained model up to the penultimate layer ---\n",
    "embedding_model = nn.Sequential(*list(model.children())[:-1])  # Remove final FC layer\n",
    "embedding_model.eval().to(device)\n",
    "\n",
    "# --- Extract embeddings from validation data ---\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, lbls in val_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Extract features from penultimate layer\n",
    "        feats = embedding_model(images)           # shape: (B, 512, 1, 1)\n",
    "        feats = feats.view(feats.size(0), -1)     # flatten to (B, 512)\n",
    "\n",
    "        embeddings.append(feats.cpu().numpy())\n",
    "        labels.extend(lbls.numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# --- Plot UMAP ---\n",
    "plot_umap(embeddings, labels, plot_3d=True, title=\"3D UMAP of Validation Features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba64867",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edab48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import shutil\n",
    "\n",
    "# === Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Adjust these paths:\n",
    "CLEAN_DATA_DIR = \"/Users/niakumar/Documents/Ultrasound_Split/train\"  # parent of class folders\n",
    "NEW_DATA_DIR = \"/Users/niakumar/Documents/New_Mask_Mult_Crop_BENIGN\"\n",
    "\n",
    "# === Image transforms (same for clean and new) ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# === Custom Dataset class to return file paths ===\n",
    "from torchvision.datasets import ImageFolder\n",
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        return img, label, path\n",
    "\n",
    "# === Load clean dataset (with filepaths if needed) ===\n",
    "clean_dataset = ImageFolderWithPaths(CLEAN_DATA_DIR, transform=transform)\n",
    "clean_loader = DataLoader(clean_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Load new dataset ===\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "class FlatImageDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.image_paths = glob.glob(os.path.join(folder, \"*.bmp\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")  # grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0, img_path  # dummy label '0' to match clean dataset output format\n",
    "    \n",
    "new_dataset = FlatImageDataset(NEW_DATA_DIR, transform=transform)\n",
    "new_loader = DataLoader(new_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Load your trained ResNet18 model ===\n",
    "# Replace this with your actual model loading code if needed\n",
    "from torchvision.models import resnet50\n",
    "model = resnet50(pretrained=False)\n",
    "model.conv1 = nn.Conv2d(\n",
    "        1,\n",
    "        64,\n",
    "        kernel_size=7,\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "model.fc = nn.Linear(in_features=2048, out_features=2)\n",
    "# Load your trained weights here, e.g.:\n",
    "# model.load_state_dict(torch.load(\"your_model_path.pth\"))\n",
    "model.load_state_dict(torch.load(\"/Users/niakumar/Documents/Ultrasound_Split/resnet_ultrasound.pt\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Create embedding extractor (all layers except final fc) ===\n",
    "embedding_model = nn.Sequential(*list(model.children())[:-1])\n",
    "embedding_model = embedding_model.to(device)\n",
    "embedding_model.eval()\n",
    "\n",
    "def extract_embeddings(dataloader):\n",
    "    embeddings = []\n",
    "    file_paths = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths in dataloader:\n",
    "            images = images.to(device)\n",
    "            feats = embedding_model(images)\n",
    "            feats = feats.view(feats.size(0), -1)\n",
    "            embeddings.append(feats.cpu().numpy())\n",
    "            file_paths.extend(paths)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings, file_paths\n",
    "\n",
    "# === Extract embeddings ===\n",
    "print(\"Extracting embeddings for clean dataset...\")\n",
    "clean_embeddings, clean_paths = extract_embeddings(clean_loader)\n",
    "\n",
    "print(\"Extracting embeddings for new dataset...\")\n",
    "new_embeddings, new_paths = extract_embeddings(new_loader)\n",
    "\n",
    "# === Combine for UMAP ===\n",
    "combined_embeddings = np.vstack([clean_embeddings, new_embeddings])\n",
    "combined_labels = [0]*len(clean_embeddings) + [1]*len(new_embeddings)  # 0=clean,1=new\n",
    "\n",
    "def plot_umap(embeddings, labels, plot_3d=False, title=\"UMAP Projection\"):\n",
    "    reducer = umap.UMAP(n_components=3 if plot_3d else 2, random_state=42)\n",
    "    embedding_umap = reducer.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    cmap_choice = 'tab10'\n",
    "    if plot_3d and embeddings.shape[1] >= 3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure(figsize=(10,7))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(\n",
    "            embedding_umap[:,0], embedding_umap[:,1], embedding_umap[:,2],\n",
    "            c=labels, cmap=cmap_choice, s=40, alpha=0.9)\n",
    "    else:\n",
    "        scatter = plt.scatter(\n",
    "            embedding_umap[:,0], embedding_umap[:,1],\n",
    "            c=labels, cmap=cmap_choice, s=40, alpha=0.9)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.colorbar(scatter, ticks=[0,1], label='Dataset (0=Clean, 1=New)')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plotting UMAP of clean vs new dataset...\")\n",
    "plot_umap(combined_embeddings, combined_labels, plot_3d=False)\n",
    "\n",
    "# === Calculate distance of new images from clean cluster center ===\n",
    "clean_center = np.mean(clean_embeddings, axis=0)\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clean_embeddings_norm = normalize(clean_embeddings)\n",
    "new_embeddings_norm = normalize(new_embeddings)\n",
    "clean_center = np.mean(clean_embeddings_norm, axis=0).reshape(1, -1)\n",
    "distances = pairwise_distances(new_embeddings_norm, clean_center)[:, 0]\n",
    "\n",
    "distances = pairwise_distances(new_embeddings, [clean_center])[:,0]\n",
    "\n",
    "# === Find top noisy candidates ===\n",
    "#top_k = 10\n",
    "#sorted_idx = np.argsort(distances)[::-1]  # descending order\n",
    "#print(f\"\\nTop {top_k} most distant (potentially noisy) images from new dataset:\")\n",
    "#for i in sorted_idx[:top_k]:\n",
    " #   print(f\"Distance: {distances[i]:.4f} - Path: {new_paths[i]}\")\n",
    "\n",
    "# === Optional: copy top noisy images to a folder for review ===\n",
    "#COPY_NOISY = True\n",
    "#NOISY_DIR = \"./noisy_candidates\"\n",
    "#if COPY_NOISY:\n",
    "\n",
    "#     os.makedirs(NOISY_DIR, exist_ok=True)\n",
    "#     for i in sorted_idx[:top_k]:\n",
    "#         src = new_paths[i]\n",
    "#         dst = os.path.join(NOISY_DIR, os.path.basename(src))\n",
    "#         shutil.copy2(src, dst)\n",
    "#     print(f\"\\nCopied top {top_k} noisy images to folder: {NOISY_DIR}\")\n",
    "\n",
    "# # print(\"Done!\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccbe35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(distances, bins=30, kde=True)\n",
    "plt.axvline(np.mean(distances), color='green', linestyle='--', label='Mean')\n",
    "plt.axvline(np.percentile(distances, 95), color='red', linestyle='--', label='95th percentile')\n",
    "plt.xlabel(\"Distance to Clean Cluster Center\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Distribution of New Image Distances\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff90dfe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sort indices by descending distance\n",
    "sorted_indices = np.argsort(distances)[::-1]\n",
    "\n",
    "print(\"Images sorted by distance (descending):\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{new_paths[idx]} = {distances[idx]:.4f}\")\n",
    "\n",
    "with open(\"distances.txt\", \"w\") as f:\n",
    "    for idx in sorted_indices:\n",
    "        f.write(f\"{distances[idx]:.4f},{new_paths[idx]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
